#!/usr/bin/env python3
import os
import time
import numpy as np
import matplotlib.pyplot as plt
from math import sqrt

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

from sklearn.decomposition import PCA
from sklearn.datasets import load_wine  # Example for DataName='Wine'

##############################################################################
# HELPER FUNCTIONS FOR MATLAB-LIKE REPORTING
##############################################################################
def matlab_print_tensor(name, tensor):
    """
    Prints info similarly to MATLAB's:
      Name   Size    Bytes  Class
    Example:
       AEDiff   178x13    18512  double
    We'll approximate 'double' if float64, 'single' if float32, etc.
    """
    shape_str = "x".join(str(s) for s in tensor.shape)
    nbytes = tensor.element_size() * tensor.numel()
    if tensor.dtype == torch.float64:
        class_str = "double"
    elif tensor.dtype == torch.float32:
        class_str = "single"
    else:
        class_str = str(tensor.dtype)

    print(f"  {name:12s}  {shape_str:>8s}  {nbytes:12d}  {class_str:6s}")

def matlab_style_report(Xrest, AEOut, iteration_idx, AEErr, start_time):
    """
    Prints lines similar to the MATLAB logs:
       - Name Size Bytes Class for AEDiff, AEOut, Xrest
       - Frobenius-norm of difference
       - L2-norm, MRSE, etc.
       - Elapsed time
    """
    print()
    print(f"  Name          Size             Bytes  Class")

    # Move to CPU for norms
    Xrest_cpu = Xrest.detach().cpu()
    AEOut_cpu = AEOut.detach().cpu()
    AEDiff = AEOut_cpu - Xrest_cpu

    # Print the three lines
    matlab_print_tensor("AEDiff", AEDiff)
    matlab_print_tensor("AEOut",  AEOut_cpu)
    matlab_print_tensor("Xrest",  Xrest_cpu)

    # Frobenius norm
    fro_norm = torch.norm(AEDiff, p='fro').item()
    print(f"\nFrobenius-norm of difference = {fro_norm:.3e}")

    # L2-norm of per-sample error
    # Tmp1 = torch.sqrt(torch.sum(AEDiff**2, dim=1))  # shape [N]
    Tmp1 = torch.sum(AEDiff**2, dim=1)  # shape [N]
    l2_of_tmp1 = torch.norm(Tmp1, p=2).item()
    print(f"L2-norm of Tmp1 = {l2_of_tmp1:.3e}")

    # MRSE = mean of per-sample L2
    mrse_val = torch.mean(torch.sqrt(Tmp1)).item()
    print(f"MRSE of Tmp1 = {mrse_val:.3e}, AEErrs(i) = {AEErr:.3e}")

    elapsed = time.time() - start_time
    print(f"Elapsed time is {elapsed:.6f} seconds.\n")


##############################################################################
# 0) Ensure we use GPU if available
##############################################################################
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

##############################################################################
# 1) Make directories and define Save paths
##############################################################################
os.makedirs("./1SymTestResults", exist_ok=True)
os.makedirs("./1SyMFigures", exist_ok=True)

DataName = 'Wine'
SavFilNam = f"./1SymTestResults/.{DataName}.pt"

# ------------------------------------------------------------------------
# 2) Load data (Wine) & define parameters
# ------------------------------------------------------------------------
data_wine = load_wine()
Data = data_wine.data  # shape [N, n0]
N, n0 = Data.shape

print(f"\nAutoECmain: Dataset {DataName} with N = {N}, n = {n0}, inits = 1.")
print(f"Finetuner DsAdam: Acc = 1.0e-09, MxIts = 100000, nbr of folds 8 ( 1/ 8).")

# Hidden-layer dimension range
nredstart = 1
nredfin   = int(0.6 * n0) + 1  # same as floor(0.6*n0)+1
nredstep  = 1
HidDims   = list(range(nredstart, nredfin + 1, nredstep))

# ------------------------------------------------------------------------
# 3) Prescale data to zero mean & range 2
# ------------------------------------------------------------------------
minD = Data.min(axis=0)
maxD = Data.max(axis=0)
if np.any(np.abs(maxD - minD) < np.sqrt(np.finfo(float).eps)):
    print("Constant variable(s) in data. Should be removed. Terminating.")
    import sys; sys.exit()

m = Data.mean(axis=0)
cofs = 2.0 / (maxD - minD)
# X will be zero-mean, featurewise range=2
X = (Data - m) * cofs  # shape [N, n0]

# ------------------------------------------------------------------------
# 4) PCA for the linear part
# ------------------------------------------------------------------------
pca_model = PCA(n_components=n0)
Y = pca_model.fit_transform(X)   # Y = XU
U = pca_model.components_.T      # shape [n0, n0]


##############################################################################
# 5) Define a 1-Sym layer in PyTorch
##############################################################################
class OneSymAutoencoder(nn.Module):
    """
    Single symmetric layer:
      hidden = act(W @ x^T)
      out    = W^T @ hidden
    where act(z) = 2/(1+exp(-2z)) - 1
    """
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        # 0.1*(2*rand -1) for initialization
        w_init = 0.1 * (2.0 * torch.rand(hidden_dim, input_dim) - 1.0)
        self.W = nn.Parameter(w_init)  # shape [hidden_dim, input_dim]

    def forward(self, x):
        """
        x: [batch_size, input_dim]
        z     = W @ x^T           -> [hidden_dim, batch_size]
        hidden= 2*sigmoid(2*z)-1  -> same shape
        out   = W^T @ hidden -> [input_dim, batch_size], then .t()
        returns out: [batch_size, input_dim]
        """
        z = torch.matmul(self.W, x.t())  # [hidden_dim, batch_size]
        hidden = 2.0 * torch.sigmoid(2.0 * z) - 1.0
        out = torch.matmul(self.W.t(), hidden).t()  # [batch_size, input_dim]
        return out


##############################################################################
# 6) Training function for single-sym feedforward net: "TrainSFFN"
##############################################################################
def train_sffn(
    model,
    Xrest,         # shape [N, n0] NumPy array
    device,
    lr=1e-3,
    max_iters=5000,
    tol=1e-9,
    batch_size=8,
    verbose=False
):
    """
    Minimizes MSE between model output and Xrest.
    Returns:
      best_model_state, best_loss, list_of_losses, num_iter
        - best_model_state: the state_dict of the best model
        - best_loss: the best (lowest) MSE found
        - list_of_losses: list of losses at each mini-batch update
        - num_iter: total number of mini-batch steps performed
    """
    # Move model to device
    model.to(device)

    # Convert data to torch on device
    Xrest_t = torch.from_numpy(Xrest).float().to(device)

    # Create DataLoader for mini-batches
    dataset = TensorDataset(Xrest_t)
    loader  = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    optimizer = optim.Adam(model.parameters(), lr=lr)
    mse_loss  = nn.MSELoss()

    best_loss = float('inf')
    best_state = None

    # We'll keep track of loss per iteration (batch update)
    loss_values = []
    num_iter = 0

    # Hard cap of 10k epochs, each epoch iterates through all batches
    for epoch in range(10000):
        for batch in loader:
            num_iter += 1
            x_batch = batch[0]  # shape [bs, n0] on device

            optimizer.zero_grad()
            out = model(x_batch)
            loss = mse_loss(out, x_batch)
            loss.backward()
            optimizer.step()

            cur_loss = loss.item()
            loss_values.append(cur_loss)

            # Track best
            if cur_loss < best_loss:
                best_loss = cur_loss
                best_state = model.state_dict()

            # Stop if below tolerance or we hit max_iters
            if best_loss < tol or num_iter >= max_iters:
                break

        if best_loss < tol or num_iter >= max_iters:
            break

    if verbose:
        print(f"TrainSFFN/Adam: final_loss={best_loss:.3e}, it={num_iter}")

    return best_state, best_loss, loss_values, num_iter


##############################################################################
# 7) Main Loop over possible nred values
##############################################################################
PCAErr = []
AEErrs = []
tic0 = time.time()

for i, nred in enumerate(HidDims, start=1):
    print(f"\nIter {i:2d}: nred = {nred:4d} (of {HidDims[-1]:4d})")

    # PCA reconstruction
    PCArecon = Y[:, :nred] @ U[:, :nred].T  # shape [N, n0]
    Xrest = X - PCArecon

    # f0: quick measure of initial MSE with random W
    model = OneSymAutoencoder(input_dim=n0, hidden_dim=nred).to(device)
    with torch.no_grad():
        Xrest_t = torch.from_numpy(Xrest).float().to(device)
        AEOut_t = model(Xrest_t)
        f0 = ((AEOut_t - Xrest_t) ** 2).mean().item()

    # Train
    t0 = time.time()
    best_state, best_funval, loss_values, num_iter = train_sffn(
        model=model,
        Xrest=Xrest,
        device=device,
        lr=1e-3,
        max_iters=5000,
        tol=1e-9,
        batch_size=64,
        verbose=False
    )
    t1 = time.time()

    # Load best weights
    model.load_state_dict(best_state)

    # Final MSE after training
    with torch.no_grad():
        AEOut_t = model(Xrest_t)
        final_mse = ((AEOut_t - Xrest_t)**2).mean().item()
        final_rmse = np.sqrt(final_mse)

    ratio = final_mse / f0 if f0 > 1e-14 else 0.0
    TrE = final_rmse
    n_params = nred * n0
    time_elapsed = t1 - t0

    # Print log with iteration count
    print(f"TrainSFFN/DsAdam: n= {n_params:3d}, f0= {f0:.3e}, f^*= {final_mse:.3e}, "
          f"f^*/f0= {ratio:.2e}, it= {num_iter}, TrE= {TrE:.3e}")
    print(f"Elapsed time is {time_elapsed:.6f} seconds.")

    # --------------------------------------------------------------------
    # STORE final RMSE for plotting
    # --------------------------------------------------------------------
    AEErrs.append(final_rmse)

    # PCA error (RMSE)
    pca_mse = ((X - PCArecon)**2).mean()
    pca_rmse = np.sqrt(pca_mse)
    PCAErr.append(pca_rmse)

    # --------------------------------------------------------------------
    # SAVE partial results
    # --------------------------------------------------------------------
    results_dict = {
        'DataName': DataName,
        'N': N,
        'n0': n0,
        'nred': nred,
        'PCAErr': PCAErr,
        'AEErrs': AEErrs
    }
    torch.save(results_dict, SavFilNam)

    # --------------------------------------------------------------------
    # PLOT the training loss curve for this dimension
    # --------------------------------------------------------------------
    plt.figure()
    plt.plot(loss_values, label=f'nred={nred}')
    plt.xlabel("Iteration (mini-batch steps)")
    plt.ylabel("MSE Loss")
    plt.title(f"Training Loss Curve (nred={nred})")
    plt.grid(True)
    plt.legend()
    plt.show()

    # --------------------------------------------------------------------
    # (NEW) DO MATLAB-STYLE PRINTS HERE (Fro-norm, MRSE, etc.)
    # --------------------------------------------------------------------
    matlab_style_report(
        Xrest_t,         # the residual input
        AEOut_t,         # final autoencoder output
        i,               # iteration index
        final_rmse,      # AEErrs(i)
        t0               # start time for iteration i
    )

toc0 = time.time()
print(f"\nTotal elapsed time = {toc0 - tic0:.4f} seconds.")

##############################################################################
# 8) Plot combined PCA vs. 1Sym across all tested nred
##############################################################################
CPalette = np.array([
    [166, 206, 227],
    [ 31, 120, 180],
    [178, 223, 138],
    [ 51, 160,  44],
    [251, 154, 153],
    [227,  26,  28]
]) / 255.0

LW = 2.5
MS = 8
FS = 14

plt.figure(figsize=(9,6))

# PCA in dashed line
plt.plot(HidDims, PCAErr, '--*', color=CPalette[0], 
         markerfacecolor=CPalette[0], linewidth=LW, markersize=MS, label="PCA")

# 1Sym in dashed line
plt.plot(HidDims, AEErrs, '--*', color=CPalette[1], 
         markerfacecolor=CPalette[1], linewidth=LW, markersize=MS, label="1Sym")

plt.legend()
plt.title(f"MRSE for {DataName}", fontsize=FS)
plt.xlabel("SqDim", fontsize=FS)
plt.ylabel("RMSE", fontsize=FS)
plt.grid(True)
plt.xticks(HidDims)
plt.tight_layout()

StoreFName = f'./1SyMFigures/{DataName}_RecErrs.png'
os.makedirs(os.path.dirname(StoreFName), exist_ok=True)

print(f"\nStoreFName = '{StoreFName}'")
plt.savefig(StoreFName, dpi=100)
plt.show()
